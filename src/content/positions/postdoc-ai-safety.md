---
title: 'Postdoctoral Researcher in AI Safety'
type: postdoc
status: open
deadline: 2025-09-01
excerpt: 'Two-year postdoctoral position investigating alignment, robustness, and safety of large-scale AI systems.'
tags:
  - AI-safety
  - alignment
  - robustness
sortOrder: 2
---

## Overview

We invite applications for a postdoctoral researcher position focused on AI safety and alignment. This two-year position (with possibility of extension) offers the opportunity to lead research on ensuring that advanced AI systems are safe, reliable, and aligned with human values.

## Responsibilities

- Lead independent research projects on AI safety topics including alignment, robustness, and interpretability
- Publish high-impact papers at leading ML and AI venues
- Mentor graduate and undergraduate students
- Help build and maintain research infrastructure for safety evaluation
- Engage with the broader AI safety research community

## Requirements

- PhD in Computer Science, Machine Learning, or a closely related field
- Strong publication record at top venues (NeurIPS, ICML, ICLR, ACL, etc.)
- Experience with large language models and/or reinforcement learning
- Interest in AI safety, alignment, or related areas
- Strong collaborative and communication skills

## What We Offer

- Competitive salary and benefits package
- Dedicated computing resources for research
- Conference travel support
- Flexible and collaborative work environment
- Opportunity to shape the direction of the lab's safety research agenda

## How to Apply

Please use the application form below to express your interest. Include a link to your CV and briefly describe your research vision for the position.
